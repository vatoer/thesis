@inproceedings{NIPS2017_3f5ee243,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Attention is All you Need},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume    = {30},
  year      = {2017}
}

@book{jm3,
  author  = {Daniel Jurafsky and James H. Martin},
  title   = {Speech and Language Processing: An Introduction to
             Natural Language Processing, Computational Linguistics,
             and Speech Recognition with Language Models},
  year    = {2025},
  url     = {https://web.stanford.edu/~jurafsky/slp3/},
  note    = {Online manuscript released January 12, 2025},
  edition = {3rd}
}

@inproceedings{NEURIPS2020_6b493230,
  author    = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {9459--9474},
  publisher = {Curran Associates, Inc.},
  title     = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@misc{gupta2024comprehensivesurveyretrievalaugmentedgeneration,
  title         = {A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions},
  author        = {Shailja Gupta and Rajesh Ranjan and Surya Narayan Singh},
  year          = {2024},
  eprint        = {2410.12837},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2410.12837}
}

@misc{jin2025searchr1trainingllmsreason,
  title         = {Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning},
  author        = {Bowen Jin and Hansi Zeng and Zhenrui Yue and Jinsung Yoon and Sercan Arik and Dong Wang and Hamed Zamani and Jiawei Han},
  year          = {2025},
  eprint        = {2503.09516},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2503.09516}
}

@online{govnet2025,
  author = {Piers Kelly},
  title  = {How Governments are Using AI: 8 Real-World Case Studies},
  year   = {2025},
  url    = {https://blog.govnet.co.uk/technology/ai-in-government-case-studies},
  note   = {Accessed: May 17, 2025}
}

@online{govsgvica2025,
  author = {{Government Technology Agency of Singapore (GovTech)}},
  title  = {Activate Public-Facing Chatbots and Serve Citizens Better with VICA},
  year   = {2025},
  url    = {https://www.tech.gov.sg/products-and-services/for-government-agencies/productivity-and-marketing/vica/},
  note   = {Last updated: May 14, 2025. Accessed: May 17, 2025}
}

@online{govsg2016,
  author = {{Government Technology Agency of Singapore (GovTech)}},
  title  = {Winning by Innovating},
  year   = {2016},
  url    = {https://www.tech.gov.sg/media/technews/winning-by-innovating/},
  note   = {Last updated: May 14, 2025. Accessed: May 17, 2025}
}

@misc{luo2025gfmraggraphfoundationmodel,
  title         = {GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation},
  author        = {Linhao Luo and Zicheng Zhao and Gholamreza Haffari and Dinh Phung and Chen Gong and Shirui Pan},
  year          = {2025},
  eprint        = {2502.01113},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR},
  url           = {https://arxiv.org/abs/2502.01113}
}

@misc{hu2024graggraphretrievalaugmentedgeneration,
  title         = {GRAG: Graph Retrieval-Augmented Generation},
  author        = {Yuntong Hu and Zhihan Lei and Zheng Zhang and Bo Pan and Chen Ling and Liang Zhao},
  year          = {2024},
  eprint        = {2405.16506},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2405.16506}
}

@misc{han2025retrievalaugmentedgenerationgraphsgraphrag,
  title         = {Retrieval-Augmented Generation with Graphs (GraphRAG)},
  author        = {Haoyu Han and Yu Wang and Harry Shomer and Kai Guo and Jiayuan Ding and Yongjia Lei and Mahantesh Halappanavar and Ryan A. Rossi and Subhabrata Mukherjee and Xianfeng Tang and Qi He and Zhigang Hua and Bo Long and Tong Zhao and Neil Shah and Amin Javari and Yinglong Xia and Jiliang Tang},
  year          = {2025},
  eprint        = {2501.00309},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR},
  url           = {https://arxiv.org/abs/2501.00309}
}

@misc{fan2025miniragextremelysimpleretrievalaugmented,
  title         = {MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation},
  author        = {Tianyu Fan and Jingyuan Wang and Xubin Ren and Chao Huang},
  year          = {2025},
  eprint        = {2501.06713},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2501.06713}
}

@misc{liang2025saferagbenchmarkingsecurityretrievalaugmented,
  title         = {SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model},
  author        = {Xun Liang and Simin Niu and Zhiyu Li and Sensen Zhang and Hanyu Wang and Feiyu Xiong and Jason Zhaoxin Fan and Bo Tang and Shichao Song and Mengwei Wang and Jiawei Yang},
  year          = {2025},
  eprint        = {2501.18636},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/abs/2501.18636}
}

@misc{singh2025agenticretrievalaugmentedgenerationsurvey,
  title         = {Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG},
  author        = {Aditi Singh and Abul Ehtesham and Saket Kumar and Tala Talaei Khoei},
  year          = {2025},
  eprint        = {2501.09136},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2501.09136}
}

@misc{zhou2025trustragenhancingrobustnesstrustworthiness,
  title         = {TrustRAG: Enhancing Robustness and Trustworthiness in RAG},
  author        = {Huichi Zhou and Kin-Hei Lee and Zhonghao Zhan and Yue Chen and Zhenhao Li and Zhaoyang Wang and Hamed Haddadi and Emine Yilmaz},
  year          = {2025},
  eprint        = {2501.00879},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2501.00879}
}

@misc{li2025enhancingretrievalaugmentedgenerationstudy,
  title         = {Enhancing Retrieval-Augmented Generation: A Study of Best Practices},
  author        = {Siran Li and Linus Stenzel and Carsten Eickhoff and Seyed Ali Bahrainian},
  year          = {2025},
  eprint        = {2501.07391},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2501.07391}
}

@misc{wang2025chainofretrievalaugmentedgeneration,
  title         = {Chain-of-Retrieval Augmented Generation},
  author        = {Liang Wang and Haonan Chen and Nan Yang and Xiaolong Huang and Zhicheng Dou and Furu Wei},
  year          = {2025},
  eprint        = {2501.14342},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR},
  url           = {https://arxiv.org/abs/2501.14342}
}

@misc{krishna2025factfetchreasonunified,
  title         = {Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation},
  author        = {Satyapriya Krishna and Kalpesh Krishna and Anhad Mohananey and Steven Schwarcz and Adam Stambler and Shyam Upadhyay and Manaal Faruqui},
  year          = {2025},
  eprint        = {2409.12941},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2409.12941}
}

@misc{qi2025long2ragevaluatinglongcontext,
  title         = {Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall},
  author        = {Zehan Qi and Rongwu Xu and Zhijiang Guo and Cunxiang Wang and Hao Zhang and Wei Xu},
  year          = {2025},
  eprint        = {2410.23000},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2410.23000}
}

@misc{zhai2024selfadaptivemultimodalretrievalaugmentedgeneration,
  title         = {Self-adaptive Multimodal Retrieval-Augmented Generation},
  author        = {Wenjia Zhai},
  year          = {2024},
  eprint        = {2410.11321},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2410.11321}
}

@online{usdosai2025,
  author = {{U.S. Department of State}},
  title  = {Department of State AI Inventory 2024},
  year   = {2025},
  url    = {https://2021-2025.state.gov/department-of-state-ai-inventory-2024/},
  note   = {Accessed: May 17, 2025}
}

@article{molaee2025,
  author    = {Molaee, Azam},
  title     = {AI Embassies: A New Frontier in Cyber Domain},
  journal   = {Journal of Cyberspace Studies},
  volume    = {9},
  number    = {1},
  pages     = {203-227},
  year      = {2025},
  publisher = {University of Tehran on behalf of the "Cyberspace Research Policy Center" and the "UNESCO Chair on Cyberspace and Culture: Dual Spacization of the World"},
  issn      = {2588-5499},
  eissn     = {2588-5502},
  doi       = {10.22059/jcss.2025.383498.1108},
  abstract  = {Background: The world is rapidly becoming more intelligent, and AI is penetrating many fields, including international affairs. Based on this, we will soon witness the emergence of a new generation of embassies, namely AI embassies.Aims: This article answers the main question: "What are the prerequisites and requirements for using AI in embassies?"Methodology: This research, conducted using a qualitative approach and socio-technical theory, examines changes around embassies and operational experiences in this area and concludes that ambassadors need to align with modern developments to succeed in their diplomatic missions.Findings: The research findings indicate that a comprehensive and accurate understanding of the developments in the host country and benefiting from AI suggestions for developing relations with the government and people of the host country are among the advantages of AI embassies. Security issues and the need for skilled human resources are some of the challenges of AI embassies. Finally, the main achievement of this article is to provide an operational framework for the responsible use of AI in embassies.Conclusion: Designing and implementing an artificial intelligence strategy, ensuring data quality and security, empowering embassy staff, and continuous monitoring and evaluation are the most critical components of this proposed framework.},
  keywords  = {AI,embassy,consular services,Cyber Security,diplomacy,virtual embassy},
  url       = {https://jcss.ut.ac.ir/article_100581.html},
  eprint    = {https://jcss.ut.ac.ir/article_100581_a63c21762f4b96d4392cdac251b8c001.pdf}
}

@article{mostafaei2025,
  author  = {Mostafaei, Hamidreza and Kordnoori, Shirin and Ostadrahimi, Mohammadmohsen and seyed agha banihashemi, Saeed and Debo, Daba},
  year    = {2025},
  month   = {02},
  pages   = {1-15},
  title   = {Applications of Artificial Intelligence in Global Diplomacy: A Review of Research and Practical Models},
  volume  = {9},
  journal = {Sustainable Futures},
  doi     = {10.1016/j.sftr.2025.100486}
}

@online{karzhevdatacamp2025,
  author    = {Stanislav Karzhev},
  title     = {Advanced RAG Techniques },
  year      = {2025},
  url       = {https://www.datacamp.com/blog/rag-advanced},
  publisher = {DataCamp},
  note      = {Accessed: May 17, 2025}
}

@online{gemma_qlora_2025,
  author = {{Google AI}},
  title  = {Fine-Tune Gemma using Hugging Face Transformers and QLoRA},
  year   = {2025},
  url    = {https://ai.google.dev/gemma/docs/core/huggingface_text_finetune_qlora},
  note   = {Accessed: May 17, 2025}
}

@online{gemma_lora_2025,
  author = {{Google AI}},
  title  = {Fine-tune Gemma in Keras using LoRA},
  year   = {2025},
  url    = {https://ai.google.dev/gemma/docs/core/lora_tuning},
  note   = {Accessed: May 17, 2025}
}

@misc{ratnakar2025qapairsassessingparameterefficient,
  title         = {Beyond QA Pairs: Assessing Parameter-Efficient Fine-Tuning for Fact Embedding in LLMs},
  author        = {Shivam Ratnakar and Abhiroop Talasila and Raghav Chamadiya and Nikhil Agarwal and Vinayak K Doifode},
  year          = {2025},
  eprint        = {2503.01131},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2503.01131}
}

@misc{sahabat_ai,
  author       = {{GoToCompany}},
  title        = {Gemma2 9B CPT Sahabat-AI v1 Instruct},
  year         = {2025},
  publisher    = {Hugging Face},
  howpublished = {\url{https://huggingface.co/GoToCompany}},
  note         = {Indonesian-focused instruction-tuned language model}
}

@misc{gemma2_sahabat_ai_v1_instruct,
  author       = {GoToCompany and AI Singapore},
  title        = {Gemma2 9B CPT Sahabat-AI v1 Instruct},
  year         = {2025},
  publisher    = {Hugging Face},
  howpublished = {\url{https://huggingface.co/GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct}},
  note         = {Indonesian-focused instruction-tuned language model}
}

@misc{llama3_sahabat_ai_v1_instruct,
  author       = {GoToCompany and AI Singapore},
  title        = {Gemma2 9B CPT Sahabat-AI v1 Instruct},
  year         = {2025},
  publisher    = {Hugging Face},
  howpublished = {\url{https://huggingface.co/GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct}},
  note         = {Indonesian-focused instruction-tuned language model}
}

@misc{koto2023largelanguagemodelspass,
  title         = {Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU},
  author        = {Fajri Koto and Nurul Aisyah and Haonan Li and Timothy Baldwin},
  year          = {2023},
  eprint        = {2310.04928},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2310.04928}
}

@misc{kemlu2025chatbot,
  author = {{Kementerian Luar Negeri Republik Indonesia}},
  title  = {Kementerian Luar Negeri dan UN Women Memperkuat Pelindungan Perempuan Pekerja Migran Indonesia melalui Inovasi Chatbot AI SARI},
  year   = {2025},
  month  = {03},
  day    = {21},
  url    = {https://kemlu.go.id/berita/kementerian-luar-negeri-dan-un-women-memperkuat-pelindungan-perempuan-pekerja-migran-indonesia-melalui-inovasi-chatbot-ai-sari?type=publication},
  note   = {Artikel di situs resmi Kementerian Luar Negeri RI, Accessed: May 17, 2025}
}

@legal{UU37_1999,
  author = {{Pemerintah Republik Indonesia}},
  title  = {Undang-Undang Republik Indonesia Nomor 37 Tahun 1999 tentang Hubungan Luar Negeri},
  year   = {1999},
  note   = {Jakarta: Sekretariat Negara Republik Indonesia},
  url    = {https://peraturan.bpk.go.id/Home/Details/44910/uu-no-37-tahun-1999},
  note   = {Accessed: May 17, 2025}
}

@misc{Keppres108_2003,
  author  = {{Pemerintah Republik Indonesia}},
  title   = {Keputusan Presiden Republik Indonesia Nomor 108 Tahun 2003 tentang Organisasi Perwakilan Republik Indonesia di Luar Negeri},
  author  = {{Pemerintah Republik Indonesia}},
  year    = {2003},
  note    = {Ditetapkan di Jakarta pada tanggal 31 Desember 2003},
  url     = {https://peraturan.bpk.go.id/Details/56472/keppres-no-108-tahun-2003},
  urldate = {2025-05-17}
}

@legal{Permenlu5_2018,
  author  = {{Kementerian Luar Negeri Republik Indonesia}},
  title   = {Peraturan Menteri Luar Negeri Republik Indonesia Nomor 5 Tahun 2018 tentang Pelindungan WNI di Luar Negeri},
  author  = {{Kementerian Luar Negeri Republik Indonesia}},
  year    = {2018},
  note    = {Jakarta: Kementerian Luar Negeri RI},
  url     = {https://peraturan.bpk.go.id/Details/139502/permenlu-no-5-tahun-2018},
  urldate = {2025-05-17}
}

@legal{UU37_1999,
  author  = {{Pemerintah Republik Indonesia}},
  title   = {Undang-Undang Republik Indonesia Nomor 37 Tahun 1999 tentang Hubungan Luar Negeri},
  year    = {1999},
  note    = {Jakarta: Sekretariat Negara Republik Indonesia},
  url     = {https://peraturan.bpk.go.id/Details/44910/uu-no-37-tahun-1999},
  urldate = {2025-05-17}
}

@legal{Keppres108_2003,
  title   = {Keputusan Presiden Republik Indonesia Nomor 108 Tahun 2003 tentang Organisasi Perwakilan RI di Luar Negeri},
  author  = {{Pemerintah Republik Indonesia}},
  year    = {2003},
  note    = {Jakarta: Sekretariat Negara Republik Indonesia},
  url     = {https://peraturan.bpk.go.id/Details/56472/keppres-no-108-tahun-2003},
  urldate = {2025-05-17}
}

@online{PeduliWNI,
  title   = {Portal Pelayanan dan Pelindungan WNI di Luar Negeri},
  author  = {{Kementerian Luar Negeri Republik Indonesia}},
  year    = {2025},
  note    = {Jakarta: Kementerian Luar Negeri RI},
  url     = {https://peduliwni.kemlu.go.id/beranda.html},
  urldate = {2025-05-17}
}

@legal{KPU301_2024,
  title   = {Keputusan Komisi Pemilihan Umum Nomor 301 Tahun 2024 tentang Perubahan Kedua atas Keputusan Komisi Pemilihan Umum Nomor 857 Tahun 2023 tentang Penetapan Rekapitulasi Daftar Pemilih Tetap Tingkat Nasional Dalam Penyelenggaraan Pemilihan Umum Tahun 2024},
  author  = {{Komisi Pemilihan Umum Republik Indonesia}},
  year    = {2024},
  note    = {Ditetapkan di Jakarta pada 04 Maret 2024 oleh Ketua KPU Hasyim Asy'ari},
  url     = {https://jdih.kpu.go.id/keputusan-kpu/detail/N0jndiSNbf2eVSfDfRDZNXRWQ01mU1hXYk8vYnJpeEpjMHJ0V3c9PQ},
  urldate = {2025-05-17}
}

@legal{UU12_2006,
  title    = {Undang-Undang Nomor 12 Tahun 2006 tentang Kewarganegaraan Republik Indonesia},
  author   = {{Pemerintah Republik Indonesia}},
  year     = {2006},
  note     = {LN.2006/NO.63, TLN NO.4634, LL SETNEG: 20 HLM},
  location = {Jakarta},
  date     = {2006-08-01},
  urldate  = {2025-05-17},
  url      = {https://peraturan.bpk.go.id/Details/40176/uu-no-12-tahun-2006},
  keywords = {Kewarganegaraan, Imigrasi, Peraturan Perundang-undangan}
}

@online{Tempo_AI_Kemlu,
  author  = {{Tempo.co}},
  title   = {Kementerian Luar Negeri akan Gunakan AI untuk Pelayanan WNI di Luar Negeri},
  year    = {2023},
  url     = {https://www.tempo.co/internasional/kementerian-luar-negeri-akan-gunakan-ai-untuk-pelayanan-wni-di-luar-negeri-1207028},
  urldate = {2025-05-17},
  note    = {Diakses pada 17 Mei 2025}
}

@inproceedings{devlin-etal-2019-bert,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  editor    = {Burstein, Jill  and
               Doran, Christy  and
               Solorio, Thamar},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1423/},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171--4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@misc{liu2019robertarobustlyoptimizedbert,
  title         = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author        = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  year          = {2019},
  eprint        = {1907.11692},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1907.11692}
}

@article{radford2018improving,
  title     = {Improving language understanding by generative pre-training},
  author    = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year      = {2018},
  publisher = {San Francisco, CA, USA}
}

@misc{ziegler2020finetuninglanguagemodelshuman,
  title         = {Fine-Tuning Language Models from Human Preferences},
  author        = {Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
  year          = {2020},
  eprint        = {1909.08593},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1909.08593}
}

@article{radford2019language,
  title   = {Language models are unsupervised multitask learners},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  volume  = {1},
  number  = {8},
  pages   = {9},
  year    = {2019}
}

@misc{brown2020languagemodelsfewshotlearners,
  title         = {Language Models are Few-Shot Learners},
  author        = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year          = {2020},
  eprint        = {2005.14165},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2005.14165}
}

@misc{gemmateam2024gemmaopenmodelsbased,
  title         = {Gemma: Open Models Based on Gemini Research and Technology},
  author        = {Gemma Team and Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivière and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and Léonard Hussenot and Pier Giuseppe Sessa and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Amélie Héliou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Clément Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Mikuła and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vlad Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Warkentin and Ludovic Peran and Minh Giang and Clément Farabet and Oriol Vinyals and Jeff Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy},
  year          = {2024},
  eprint        = {2403.08295},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2403.08295}
}

@misc{lialin2024scalingscaleupguide,
  title         = {Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning},
  author        = {Vladislav Lialin and Vijeta Deshpande and Xiaowei Yao and Anna Rumshisky},
  year          = {2024},
  eprint        = {2303.15647},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2303.15647}
}

@inproceedings{stap-etal-2024-fine,
  title     = {The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing {LLM} Abilities},
  author    = {Stap, David  and
               Hasler, Eva  and
               Byrne, Bill  and
               Monz, Christof  and
               Tran, Ke},
  editor    = {Ku, Lun-Wei  and
               Martins, Andre  and
               Srikumar, Vivek},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = aug,
  year      = {2024},
  address   = {Bangkok, Thailand},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.acl-long.336/},
  doi       = {10.18653/v1/2024.acl-long.336},
  pages     = {6189--6206},
  abstract  = {Fine-tuning large language models (LLMs) for machine translation has shown improvements in overall translation quality. However, it is unclear what is the impact of fine-tuning on desirable LLM behaviors that are not present in neural machine translation models, such as steerability, inherent document-level translation abilities, and the ability to produce less literal translations. We perform an extensive translation evaluation on the LLaMA and Falcon family of models with model size ranging from 7 billion up to 65 billion parameters.Our results show that while fine-tuning improves the general translation quality of LLMs, several abilities degrade. In particular, we observe a decline in the ability to perform formality steering, to produce technical translations through few-shot examples, and to perform document-level translation. On the other hand, we observe that the model produces less literal translations after fine-tuning on parallel data. We show that by including monolingual data as part of the fine-tuning data we can maintain the abilities while simultaneously enhancing overall translation quality. Our findings emphasize the need for fine-tuning strategies that preserve the benefits of LLMs for machine translation.}
}


@inproceedings{pmlr-v235-song24e,
  title     = {Sparse is Enough in Fine-tuning Pre-trained Large Language Models},
  author    = {Song, Weixi and Li, Zuchao and Zhang, Lefei and Zhao, Hai and Du, Bo},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  pages     = {46121--46135},
  year      = {2024},
  editor    = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume    = {235},
  series    = {Proceedings of Machine Learning Research},
  month     = {21--27 Jul},
  publisher = {PMLR},
  pdf       = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/song24e/song24e.pdf},
  url       = {https://proceedings.mlr.press/v235/song24e.html},
  abstract  = {With the prevalence of pre-training-fine-tuning paradigm, how to efficiently adapt the pre-trained model to the downstream tasks has been an intriguing issue. $\textbf{P}$arameter-$\textbf{E}$fficient $\textbf{F}$ine-$\textbf{T}$uning(PEFT) methods have been proposed for low-cost adaptation. Although PEFT has demonstrated effectiveness and been widely applied, the underlying principles are still unclear. In this paper, we adopt the PAC-Bayesian generalization error bound, viewing pre-training as a shift of prior distribution which leads to a tighter bound for generalization error. We validate this shift from the perspectives of oscillations in the loss landscape and the quasi-sparsity in gradient distribution. Based on this, we propose a gradient-based sparse fine-tuning algorithm, named $\textbf{S}$parse $\textbf{I}$ncrement $\textbf{F}$ine-$\textbf{T}$uning(SIFT), and validate its effectiveness on a range of tasks including the GLUE Benchmark and Instruction-tuning. The code is accessible at https://github.com/song-wx/SIFT/.}
}

@misc{saroufim2025neurips2023llmefficiency,
  title         = {NeurIPS 2023 LLM Efficiency Fine-tuning Competition},
  author        = {Mark Saroufim and Yotam Perlitz and Leshem Choshen and Luca Antiga and Greg Bowyer and Christian Puhrsch and Driss Guessous and Supriya Rao and Geeta Chauhan and Ashvini Kumar and Jindal Pawan Kumar and Rajpoot Ankur Parikh and Joe Isaacson and Weiwei Yang},
  year          = {2025},
  eprint        = {2503.13507},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2503.13507}
}

@misc{weng2024navigatinglandscapelargelanguage,
  title         = {Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies},
  author        = {Benjue Weng},
  year          = {2024},
  eprint        = {2404.09022},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2404.09022}
}

@misc{openai2024gpt4technicalreport,
  title         = {GPT-4 Technical Report},
  author        = {OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
  year          = {2024},
  eprint        = {2303.08774},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2303.08774}
}

@misc{touvron2023llama2openfoundation,
  title         = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author        = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  year          = {2023},
  eprint        = {2307.09288},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2307.09288}
}

@misc{anthropic2025claude37,
  title  = {Claude 3.7 Sonnet and Claude Code},
  author = {{Anthropic}},
  year   = {2025},
  month  = feb,
  url    = {https://www.anthropic.com/news/claude-3-7-sonnet},
  note   = {Accessed: 2025-05-17}
}

@misc{geminiteam2025geminifamilyhighlycapable,
  title         = {Gemini: A Family of Highly Capable Multimodal Models},
  author        = {Gemini Team and Rohan Anil and Sebastian Borgeaud and Jean-Baptiste Alayrac and Jiahui Yu and Radu Soricut and Johan Schalkwyk and Andrew M. Dai and Anja Hauth and Katie Millican and David Silver and Melvin Johnson and Ioannis Antonoglou and Julian Schrittwieser and Amelia Glaese and Jilin Chen and Emily Pitler and Timothy Lillicrap and Angeliki Lazaridou and Orhan Firat and James Molloy and Michael Isard and Paul R. Barham and Tom Hennigan and Benjamin Lee and Fabio Viola and Malcolm Reynolds and Yuanzhong Xu and Ryan Doherty and Eli Collins and Clemens Meyer and Eliza Rutherford and Erica Moreira and Kareem Ayoub and Megha Goel and Jack Krawczyk and Cosmo Du and Ed Chi and Heng-Tze Cheng and Eric Ni and Purvi Shah and Patrick Kane and Betty Chan and Manaal Faruqui and Aliaksei Severyn and Hanzhao Lin and YaGuang Li and Yong Cheng and Abe Ittycheriah and Mahdis Mahdieh and Mia Chen and Pei Sun and Dustin Tran and Sumit Bagri and Balaji Lakshminarayanan and Jeremiah Liu and Andras Orban and Fabian Güra and Hao Zhou and Xinying Song and Aurelien Boffy and Harish Ganapathy and Steven Zheng and HyunJeong Choe and Ágoston Weisz and Tao Zhu and Yifeng Lu and Siddharth Gopal and Jarrod Kahn and Maciej Kula and Jeff Pitman and Rushin Shah and Emanuel Taropa and Majd Al Merey and Martin Baeuml and Zhifeng Chen and Laurent El Shafey and Yujing Zhang and Olcan Sercinoglu and George Tucker and Enrique Piqueras and Maxim Krikun and Iain Barr and Nikolay Savinov and Ivo Danihelka and Becca Roelofs and Anaïs White and Anders Andreassen and Tamara von Glehn and Lakshman Yagati and Mehran Kazemi and Lucas Gonzalez and Misha Khalman and Jakub Sygnowski and Alexandre Frechette and Charlotte Smith and Laura Culp and Lev Proleev and Yi Luan and Xi Chen and James Lottes and Nathan Schucher and Federico Lebron and Alban Rrustemi and Natalie Clay and Phil Crone and Tomas Kocisky and Jeffrey Zhao and Bartek Perz and Dian Yu and Heidi Howard and Adam Bloniarz and Jack W. Rae and Han Lu and Laurent Sifre and Marcello Maggioni and Fred Alcober and Dan Garrette and Megan Barnes and Shantanu Thakoor and Jacob Austin and Gabriel Barth-Maron and William Wong and Rishabh Joshi and Rahma Chaabouni and Deeni Fatiha and Arun Ahuja and Gaurav Singh Tomar and Evan Senter and Martin Chadwick and Ilya Kornakov and Nithya Attaluri and Iñaki Iturrate and Ruibo Liu and Yunxuan Li and Sarah Cogan and Jeremy Chen and Chao Jia and Chenjie Gu and Qiao Zhang and Jordan Grimstad and Ale Jakse Hartman and Xavier Garcia and Thanumalayan Sankaranarayana Pillai and Jacob Devlin and Michael Laskin and Diego de Las Casas and Dasha Valter and Connie Tao and Lorenzo Blanco and Adrià Puigdomènech Badia and David Reitter and Mianna Chen and Jenny Brennan and Clara Rivera and Sergey Brin and Shariq Iqbal and Gabriela Surita and Jane Labanowski and Abhi Rao and Stephanie Winkler and Emilio Parisotto and Yiming Gu and Kate Olszewska and Ravi Addanki and Antoine Miech and Annie Louis and Denis Teplyashin and Geoff Brown and Elliot Catt and Jan Balaguer and Jackie Xiang and Pidong Wang and Zoe Ashwood and Anton Briukhov and Albert Webson and Sanjay Ganapathy and Smit Sanghavi and Ajay Kannan and Ming-Wei Chang and Axel Stjerngren and Josip Djolonga and Yuting Sun and Ankur Bapna and Matthew Aitchison and Pedram Pejman and Henryk Michalewski and Tianhe Yu and Cindy Wang and Juliette Love and Junwhan Ahn and Dawn Bloxwich and Kehang Han and Peter Humphreys and Thibault Sellam and James Bradbury and Varun Godbole and Sina Samangooei and Bogdan Damoc and Alex Kaskasoli and Sébastien M. R. Arnold and Vijay Vasudevan and Shubham Agrawal and Jason Riesa and Dmitry Lepikhin and Richard Tanburn and Srivatsan Srinivasan and Hyeontaek Lim and Sarah Hodkinson and Pranav Shyam and Johan Ferret and Steven Hand and Ankush Garg and Tom Le Paine and Jian Li and Yujia Li and Minh Giang and Alexander Neitz and Zaheer Abbas and Sarah York and Machel Reid and Elizabeth Cole and Aakanksha Chowdhery and Dipanjan Das and Dominika Rogozińska and Vitaliy Nikolaev and Pablo Sprechmann and Zachary Nado and Lukas Zilka and Flavien Prost and Luheng He and Marianne Monteiro and Gaurav Mishra and Chris Welty and Josh Newlan and Dawei Jia and Miltiadis Allamanis and Clara Huiyi Hu and Raoul de Liedekerke and Justin Gilmer and Carl Saroufim and Shruti Rijhwani and Shaobo Hou and Disha Shrivastava and Anirudh Baddepudi and Alex Goldin and Adnan Ozturel and Albin Cassirer and Yunhan Xu and Daniel Sohn and Devendra Sachan and Reinald Kim Amplayo and Craig Swanson and Dessie Petrova and Shashi Narayan and Arthur Guez and Siddhartha Brahma and Jessica Landon and Miteyan Patel and Ruizhe Zhao and Kevin Villela and Luyu Wang and Wenhao Jia and Matthew Rahtz and Mai Giménez and Legg Yeung and James Keeling and Petko Georgiev and Diana Mincu and Boxi Wu and Salem Haykal and Rachel Saputro and Kiran Vodrahalli and James Qin and Zeynep Cankara and Abhanshu Sharma and Nick Fernando and Will Hawkins and Behnam Neyshabur and Solomon Kim and Adrian Hutter and Priyanka Agrawal and Alex Castro-Ros and George van den Driessche and Tao Wang and Fan Yang and Shuo-yiin Chang and Paul Komarek and Ross McIlroy and Mario Lučić and Guodong Zhang and Wael Farhan and Michael Sharman and Paul Natsev and Paul Michel and Yamini Bansal and Siyuan Qiao and Kris Cao and Siamak Shakeri and Christina Butterfield and Justin Chung and Paul Kishan Rubenstein and Shivani Agrawal and Arthur Mensch and Kedar Soparkar and Karel Lenc and Timothy Chung and Aedan Pope and Loren Maggiore and Jackie Kay and Priya Jhakra and Shibo Wang and Joshua Maynez and Mary Phuong and Taylor Tobin and Andrea Tacchetti and Maja Trebacz and Kevin Robinson and Yash Katariya and Sebastian Riedel and Paige Bailey and Kefan Xiao and Nimesh Ghelani and Lora Aroyo and Ambrose Slone and Neil Houlsby and Xuehan Xiong and Zhen Yang and Elena Gribovskaya and Jonas Adler and Mateo Wirth and Lisa Lee and Music Li and Thais Kagohara and Jay Pavagadhi and Sophie Bridgers and Anna Bortsova and Sanjay Ghemawat and Zafarali Ahmed and Tianqi Liu and Richard Powell and Vijay Bolina and Mariko Iinuma and Polina Zablotskaia and James Besley and Da-Woon Chung and Timothy Dozat and Ramona Comanescu and Xiance Si and Jeremy Greer and Guolong Su and Martin Polacek and Raphaël Lopez Kaufman and Simon Tokumine and Hexiang Hu and Elena Buchatskaya and Yingjie Miao and Mohamed Elhawaty and Aditya Siddhant and Nenad Tomasev and Jinwei Xing and Christina Greer and Helen Miller and Shereen Ashraf and Aurko Roy and Zizhao Zhang and Ada Ma and Angelos Filos and Milos Besta and Rory Blevins and Ted Klimenko and Chih-Kuan Yeh and Soravit Changpinyo and Jiaqi Mu and Oscar Chang and Mantas Pajarskas and Carrie Muir and Vered Cohen and Charline Le Lan and Krishna Haridasan and Amit Marathe and Steven Hansen and Sholto Douglas and Rajkumar Samuel and Mingqiu Wang and Sophia Austin and Chang Lan and Jiepu Jiang and Justin Chiu and Jaime Alonso Lorenzo and Lars Lowe Sjösund and Sébastien Cevey and Zach Gleicher and Thi Avrahami and Anudhyan Boral and Hansa Srinivasan and Vittorio Selo and Rhys May and Konstantinos Aisopos and Léonard Hussenot and Livio Baldini Soares and Kate Baumli and Michael B. Chang and Adrià Recasens and Ben Caine and Alexander Pritzel and Filip Pavetic and Fabio Pardo and Anita Gergely and Justin Frye and Vinay Ramasesh and Dan Horgan and Kartikeya Badola and Nora Kassner and Subhrajit Roy and Ethan Dyer and Víctor Campos Campos and Alex Tomala and Yunhao Tang and Dalia El Badawy and Elspeth White and Basil Mustafa and Oran Lang and Abhishek Jindal and Sharad Vikram and Zhitao Gong and Sergi Caelles and Ross Hemsley and Gregory Thornton and Fangxiaoyu Feng and Wojciech Stokowiec and Ce Zheng and Phoebe Thacker and Çağlar Ünlü and Zhishuai Zhang and Mohammad Saleh and James Svensson and Max Bileschi and Piyush Patil and Ankesh Anand and Roman Ring and Katerina Tsihlas and Arpi Vezer and Marco Selvi and Toby Shevlane and Mikel Rodriguez and Tom Kwiatkowski and Samira Daruki and Keran Rong and Allan Dafoe and Nicholas FitzGerald and Keren Gu-Lemberg and Mina Khan and Lisa Anne Hendricks and Marie Pellat and Vladimir Feinberg and James Cobon-Kerr and Tara Sainath and Maribeth Rauh and Sayed Hadi Hashemi and Richard Ives and Yana Hasson and Eric Noland and Yuan Cao and Nathan Byrd and Le Hou and Qingze Wang and Thibault Sottiaux and Michela Paganini and Jean-Baptiste Lespiau and Alexandre Moufarek and Samer Hassan and Kaushik Shivakumar and Joost van Amersfoort and Amol Mandhane and Pratik Joshi and Anirudh Goyal and Matthew Tung and Andrew Brock and Hannah Sheahan and Vedant Misra and Cheng Li and Nemanja Rakićević and Mostafa Dehghani and Fangyu Liu and Sid Mittal and Junhyuk Oh and Seb Noury and Eren Sezener and Fantine Huot and Matthew Lamm and Nicola De Cao and Charlie Chen and Sidharth Mudgal and Romina Stella and Kevin Brooks and Gautam Vasudevan and Chenxi Liu and Mainak Chain and Nivedita Melinkeri and Aaron Cohen and Venus Wang and Kristie Seymore and Sergey Zubkov and Rahul Goel and Summer Yue and Sai Krishnakumaran and Brian Albert and Nate Hurley and Motoki Sano and Anhad Mohananey and Jonah Joughin and Egor Filonov and Tomasz Kępa and Yomna Eldawy and Jiawern Lim and Rahul Rishi and Shirin Badiezadegan and Taylor Bos and Jerry Chang and Sanil Jain and Sri Gayatri Sundara Padmanabhan and Subha Puttagunta and Kalpesh Krishna and Leslie Baker and Norbert Kalb and Vamsi Bedapudi and Adam Kurzrok and Shuntong Lei and Anthony Yu and Oren Litvin and Xiang Zhou and Zhichun Wu and Sam Sobell and Andrea Siciliano and Alan Papir and Robby Neale and Jonas Bragagnolo and Tej Toor and Tina Chen and Valentin Anklin and Feiran Wang and Richie Feng and Milad Gholami and Kevin Ling and Lijuan Liu and Jules Walter and Hamid Moghaddam and Arun Kishore and Jakub Adamek and Tyler Mercado and Jonathan Mallinson and Siddhinita Wandekar and Stephen Cagle and Eran Ofek and Guillermo Garrido and Clemens Lombriser and Maksim Mukha and Botu Sun and Hafeezul Rahman Mohammad and Josip Matak and Yadi Qian and Vikas Peswani and Pawel Janus and Quan Yuan and Leif Schelin and Oana David and Ankur Garg and Yifan He and Oleksii Duzhyi and Anton Älgmyr and Timothée Lottaz and Qi Li and Vikas Yadav and Luyao Xu and Alex Chinien and Rakesh Shivanna and Aleksandr Chuklin and Josie Li and Carrie Spadine and Travis Wolfe and Kareem Mohamed and Subhabrata Das and Zihang Dai and Kyle He and Daniel von Dincklage and Shyam Upadhyay and Akanksha Maurya and Luyan Chi and Sebastian Krause and Khalid Salama and Pam G Rabinovitch and Pavan Kumar Reddy M and Aarush Selvan and Mikhail Dektiarev and Golnaz Ghiasi and Erdem Guven and Himanshu Gupta and Boyi Liu and Deepak Sharma and Idan Heimlich Shtacher and Shachi Paul and Oscar Akerlund and François-Xavier Aubet and Terry Huang and Chen Zhu and Eric Zhu and Elico Teixeira and Matthew Fritze and Francesco Bertolini and Liana-Eleonora Marinescu and Martin Bölle and Dominik Paulus and Khyatti Gupta and Tejasi Latkar and Max Chang and Jason Sanders and Roopa Wilson and Xuewei Wu and Yi-Xuan Tan and Lam Nguyen Thiet and Tulsee Doshi and Sid Lall and Swaroop Mishra and Wanming Chen and Thang Luong and Seth Benjamin and Jasmine Lee and Ewa Andrejczuk and Dominik Rabiej and Vipul Ranjan and Krzysztof Styrc and Pengcheng Yin and Jon Simon and Malcolm Rose Harriott and Mudit Bansal and Alexei Robsky and Geoff Bacon and David Greene and Daniil Mirylenka and Chen Zhou and Obaid Sarvana and Abhimanyu Goyal and Samuel Andermatt and Patrick Siegler and Ben Horn and Assaf Israel and Francesco Pongetti and Chih-Wei "Louis" Chen and Marco Selvatici and Pedro Silva and Kathie Wang and Jackson Tolins and Kelvin Guu and Roey Yogev and Xiaochen Cai and Alessandro Agostini and Maulik Shah and Hung Nguyen and Noah Ó Donnaile and Sébastien Pereira and Linda Friso and Adam Stambler and Adam Kurzrok and Chenkai Kuang and Yan Romanikhin and Mark Geller and ZJ Yan and Kane Jang and Cheng-Chun Lee and Wojciech Fica and Eric Malmi and Qijun Tan and Dan Banica and Daniel Balle and Ryan Pham and Yanping Huang and Diana Avram and Hongzhi Shi and Jasjot Singh and Chris Hidey and Niharika Ahuja and Pranab Saxena and Dan Dooley and Srividya Pranavi Potharaju and Eileen O'Neill and Anand Gokulchandran and Ryan Foley and Kai Zhao and Mike Dusenberry and Yuan Liu and Pulkit Mehta and Ragha Kotikalapudi and Chalence Safranek-Shrader and Andrew Goodman and Joshua Kessinger and Eran Globen and Prateek Kolhar and Chris Gorgolewski and Ali Ibrahim and Yang Song and Ali Eichenbaum and Thomas Brovelli and Sahitya Potluri and Preethi Lahoti and Cip Baetu and Ali Ghorbani and Charles Chen and Andy Crawford and Shalini Pal and Mukund Sridhar and Petru Gurita and Asier Mujika and Igor Petrovski and Pierre-Louis Cedoz and Chenmei Li and Shiyuan Chen and Niccolò Dal Santo and Siddharth Goyal and Jitesh Punjabi and Karthik Kappaganthu and Chester Kwak and Pallavi LV and Sarmishta Velury and Himadri Choudhury and Jamie Hall and Premal Shah and Ricardo Figueira and Matt Thomas and Minjie Lu and Ting Zhou and Chintu Kumar and Thomas Jurdi and Sharat Chikkerur and Yenai Ma and Adams Yu and Soo Kwak and Victor Ähdel and Sujeevan Rajayogam and Travis Choma and Fei Liu and Aditya Barua and Colin Ji and Ji Ho Park and Vincent Hellendoorn and Alex Bailey and Taylan Bilal and Huanjie Zhou and Mehrdad Khatir and Charles Sutton and Wojciech Rzadkowski and Fiona Macintosh and Roopali Vij and Konstantin Shagin and Paul Medina and Chen Liang and Jinjing Zhou and Pararth Shah and Yingying Bi and Attila Dankovics and Shipra Banga and Sabine Lehmann and Marissa Bredesen and Zifan Lin and John Eric Hoffmann and Jonathan Lai and Raynald Chung and Kai Yang and Nihal Balani and Arthur Bražinskas and Andrei Sozanschi and Matthew Hayes and Héctor Fernández Alcalde and Peter Makarov and Will Chen and Antonio Stella and Liselotte Snijders and Michael Mandl and Ante Kärrman and Paweł Nowak and Xinyi Wu and Alex Dyck and Krishnan Vaidyanathan and Raghavender R and Jessica Mallet and Mitch Rudominer and Eric Johnston and Sushil Mittal and Akhil Udathu and Janara Christensen and Vishal Verma and Zach Irving and Andreas Santucci and Gamaleldin Elsayed and Elnaz Davoodi and Marin Georgiev and Ian Tenney and Nan Hua and Geoffrey Cideron and Edouard Leurent and Mahmoud Alnahlawi and Ionut Georgescu and Nan Wei and Ivy Zheng and Dylan Scandinaro and Heinrich Jiang and Jasper Snoek and Mukund Sundararajan and Xuezhi Wang and Zack Ontiveros and Itay Karo and Jeremy Cole and Vinu Rajashekhar and Lara Tumeh and Eyal Ben-David and Rishub Jain and Jonathan Uesato and Romina Datta and Oskar Bunyan and Shimu Wu and John Zhang and Piotr Stanczyk and Ye Zhang and David Steiner and Subhajit Naskar and Michael Azzam and Matthew Johnson and Adam Paszke and Chung-Cheng Chiu and Jaume Sanchez Elias and Afroz Mohiuddin and Faizan Muhammad and Jin Miao and Andrew Lee and Nino Vieillard and Jane Park and Jiageng Zhang and Jeff Stanway and Drew Garmon and Abhijit Karmarkar and Zhe Dong and Jong Lee and Aviral Kumar and Luowei Zhou and Jonathan Evens and William Isaac and Geoffrey Irving and Edward Loper and Michael Fink and Isha Arkatkar and Nanxin Chen and Izhak Shafran and Ivan Petrychenko and Zhe Chen and Johnson Jia and Anselm Levskaya and Zhenkai Zhu and Peter Grabowski and Yu Mao and Alberto Magni and Kaisheng Yao and Javier Snaider and Norman Casagrande and Evan Palmer and Paul Suganthan and Alfonso Castaño and Irene Giannoumis and Wooyeol Kim and Mikołaj Rybiński and Ashwin Sreevatsa and Jennifer Prendki and David Soergel and Adrian Goedeckemeyer and Willi Gierke and Mohsen Jafari and Meenu Gaba and Jeremy Wiesner and Diana Gage Wright and Yawen Wei and Harsha Vashisht and Yana Kulizhskaya and Jay Hoover and Maigo Le and Lu Li and Chimezie Iwuanyanwu and Lu Liu and Kevin Ramirez and Andrey Khorlin and Albert Cui and Tian LIN and Marcus Wu and Ricardo Aguilar and Keith Pallo and Abhishek Chakladar and Ginger Perng and Elena Allica Abellan and Mingyang Zhang and Ishita Dasgupta and Nate Kushman and Ivo Penchev and Alena Repina and Xihui Wu and Tom van der Weide and Priya Ponnapalli and Caroline Kaplan and Jiri Simsa and Shuangfeng Li and Olivier Dousse and Fan Yang and Jeff Piper and Nathan Ie and Rama Pasumarthi and Nathan Lintz and Anitha Vijayakumar and Daniel Andor and Pedro Valenzuela and Minnie Lui and Cosmin Paduraru and Daiyi Peng and Katherine Lee and Shuyuan Zhang and Somer Greene and Duc Dung Nguyen and Paula Kurylowicz and Cassidy Hardin and Lucas Dixon and Lili Janzer and Kiam Choo and Ziqiang Feng and Biao Zhang and Achintya Singhal and Dayou Du and Dan McKinnon and Natasha Antropova and Tolga Bolukbasi and Orgad Keller and David Reid and Daniel Finchelstein and Maria Abi Raad and Remi Crocker and Peter Hawkins and Robert Dadashi and Colin Gaffney and Ken Franko and Anna Bulanova and Rémi Leblond and Shirley Chung and Harry Askham and Luis C. Cobo and Kelvin Xu and Felix Fischer and Jun Xu and Christina Sorokin and Chris Alberti and Chu-Cheng Lin and Colin Evans and Alek Dimitriev and Hannah Forbes and Dylan Banarse and Zora Tung and Mark Omernick and Colton Bishop and Rachel Sterneck and Rohan Jain and Jiawei Xia and Ehsan Amid and Francesco Piccinno and Xingyu Wang and Praseem Banzal and Daniel J. Mankowitz and Alex Polozov and Victoria Krakovna and Sasha Brown and MohammadHossein Bateni and Dennis Duan and Vlad Firoiu and Meghana Thotakuri and Tom Natan and Matthieu Geist and Ser tan Girgin and Hui Li and Jiayu Ye and Ofir Roval and Reiko Tojo and Michael Kwong and James Lee-Thorp and Christopher Yew and Danila Sinopalnikov and Sabela Ramos and John Mellor and Abhishek Sharma and Kathy Wu and David Miller and Nicolas Sonnerat and Denis Vnukov and Rory Greig and Jennifer Beattie and Emily Caveness and Libin Bai and Julian Eisenschlos and Alex Korchemniy and Tomy Tsai and Mimi Jasarevic and Weize Kong and Phuong Dao and Zeyu Zheng and Frederick Liu and Fan Yang and Rui Zhu and Tian Huey Teh and Jason Sanmiya and Evgeny Gladchenko and Nejc Trdin and Daniel Toyama and Evan Rosen and Sasan Tavakkol and Linting Xue and Chen Elkind and Oliver Woodman and John Carpenter and George Papamakarios and Rupert Kemp and Sushant Kafle and Tanya Grunina and Rishika Sinha and Alice Talbert and Diane Wu and Denese Owusu-Afriyie and Cosmo Du and Chloe Thornton and Jordi Pont-Tuset and Pradyumna Narayana and Jing Li and Saaber Fatehi and John Wieting and Omar Ajmeri and Benigno Uria and Yeongil Ko and Laura Knight and Amélie Héliou and Ning Niu and Shane Gu and Chenxi Pang and Yeqing Li and Nir Levine and Ariel Stolovich and Rebeca Santamaria-Fernandez and Sonam Goenka and Wenny Yustalim and Robin Strudel and Ali Elqursh and Charlie Deck and Hyo Lee and Zonglin Li and Kyle Levin and Raphael Hoffmann and Dan Holtmann-Rice and Olivier Bachem and Sho Arora and Christy Koh and Soheil Hassas Yeganeh and Siim Põder and Mukarram Tariq and Yanhua Sun and Lucian Ionita and Mojtaba Seyedhosseini and Pouya Tafti and Zhiyu Liu and Anmol Gulati and Jasmine Liu and Xinyu Ye and Bart Chrzaszcz and Lily Wang and Nikhil Sethi and Tianrun Li and Ben Brown and Shreya Singh and Wei Fan and Aaron Parisi and Joe Stanton and Vinod Koverkathu and Christopher A. Choquette-Choo and Yunjie Li and TJ Lu and Abe Ittycheriah and Prakash Shroff and Mani Varadarajan and Sanaz Bahargam and Rob Willoughby and David Gaddy and Guillaume Desjardins and Marco Cornero and Brona Robenek and Bhavishya Mittal and Ben Albrecht and Ashish Shenoy and Fedor Moiseev and Henrik Jacobsson and Alireza Ghaffarkhah and Morgane Rivière and Alanna Walton and Clément Crepy and Alicia Parrish and Zongwei Zhou and Clement Farabet and Carey Radebaugh and Praveen Srinivasan and Claudia van der Salm and Andreas Fidjeland and Salvatore Scellato and Eri Latorre-Chimoto and Hanna Klimczak-Plucińska and David Bridson and Dario de Cesare and Tom Hudson and Piermaria Mendolicchio and Lexi Walker and Alex Morris and Matthew Mauger and Alexey Guseynov and Alison Reid and Seth Odoom and Lucia Loher and Victor Cotruta and Madhavi Yenugula and Dominik Grewe and Anastasia Petrushkina and Tom Duerig and Antonio Sanchez and Steve Yadlowsky and Amy Shen and Amir Globerson and Lynette Webb and Sahil Dua and Dong Li and Surya Bhupatiraju and Dan Hurt and Haroon Qureshi and Ananth Agarwal and Tomer Shani and Matan Eyal and Anuj Khare and Shreyas Rammohan Belle and Lei Wang and Chetan Tekur and Mihir Sanjay Kale and Jinliang Wei and Ruoxin Sang and Brennan Saeta and Tyler Liechty and Yi Sun and Yao Zhao and Stephan Lee and Pandu Nayak and Doug Fritz and Manish Reddy Vuyyuru and John Aslanides and Nidhi Vyas and Martin Wicke and Xiao Ma and Evgenii Eltyshev and Nina Martin and Hardie Cate and James Manyika and Keyvan Amiri and Yelin Kim and Xi Xiong and Kai Kang and Florian Luisier and Nilesh Tripuraneni and David Madras and Mandy Guo and Austin Waters and Oliver Wang and Joshua Ainslie and Jason Baldridge and Han Zhang and Garima Pruthi and Jakob Bauer and Feng Yang and Riham Mansour and Jason Gelman and Yang Xu and George Polovets and Ji Liu and Honglong Cai and Warren Chen and XiangHai Sheng and Emily Xue and Sherjil Ozair and Christof Angermueller and Xiaowei Li and Anoop Sinha and Weiren Wang and Julia Wiesinger and Emmanouil Koukoumidis and Yuan Tian and Anand Iyer and Madhu Gurumurthy and Mark Goldenson and Parashar Shah and MK Blake and Hongkun Yu and Anthony Urbanowicz and Jennimaria Palomaki and Chrisantha Fernando and Ken Durden and Harsh Mehta and Nikola Momchev and Elahe Rahimtoroghi and Maria Georgaki and Amit Raul and Sebastian Ruder and Morgan Redshaw and Jinhyuk Lee and Denny Zhou and Komal Jalan and Dinghua Li and Blake Hechtman and Parker Schuh and Milad Nasr and Kieran Milan and Vladimir Mikulik and Juliana Franco and Tim Green and Nam Nguyen and Joe Kelley and Aroma Mahendru and Andrea Hu and Joshua Howland and Ben Vargas and Jeffrey Hui and Kshitij Bansal and Vikram Rao and Rakesh Ghiya and Emma Wang and Ke Ye and Jean Michel Sarr and Melanie Moranski Preston and Madeleine Elish and Steve Li and Aakash Kaku and Jigar Gupta and Ice Pasupat and Da-Cheng Juan and Milan Someswar and Tejvi M. and Xinyun Chen and Aida Amini and Alex Fabrikant and Eric Chu and Xuanyi Dong and Amruta Muthal and Senaka Buthpitiya and Sarthak Jauhari and Nan Hua and Urvashi Khandelwal and Ayal Hitron and Jie Ren and Larissa Rinaldi and Shahar Drath and Avigail Dabush and Nan-Jiang Jiang and Harshal Godhia and Uli Sachs and Anthony Chen and Yicheng Fan and Hagai Taitelbaum and Hila Noga and Zhuyun Dai and James Wang and Chen Liang and Jenny Hamer and Chun-Sung Ferng and Chenel Elkind and Aviel Atias and Paulina Lee and Vít Listík and Mathias Carlen and Jan van de Kerkhof and Marcin Pikus and Krunoslav Zaher and Paul Müller and Sasha Zykova and Richard Stefanec and Vitaly Gatsko and Christoph Hirnschall and Ashwin Sethi and Xingyu Federico Xu and Chetan Ahuja and Beth Tsai and Anca Stefanoiu and Bo Feng and Keshav Dhandhania and Manish Katyal and Akshay Gupta and Atharva Parulekar and Divya Pitta and Jing Zhao and Vivaan Bhatia and Yashodha Bhavnani and Omar Alhadlaq and Xiaolin Li and Peter Danenberg and Dennis Tu and Alex Pine and Vera Filippova and Abhipso Ghosh and Ben Limonchik and Bhargava Urala and Chaitanya Krishna Lanka and Derik Clive and Yi Sun and Edward Li and Hao Wu and Kevin Hongtongsak and Ianna Li and Kalind Thakkar and Kuanysh Omarov and Kushal Majmundar and Michael Alverson and Michael Kucharski and Mohak Patel and Mudit Jain and Maksim Zabelin and Paolo Pelagatti and Rohan Kohli and Saurabh Kumar and Joseph Kim and Swetha Sankar and Vineet Shah and Lakshmi Ramachandruni and Xiangkai Zeng and Ben Bariach and Laura Weidinger and Tu Vu and Alek Andreev and Antoine He and Kevin Hui and Sheleem Kashem and Amar Subramanya and Sissie Hsiao and Demis Hassabis and Koray Kavukcuoglu and Adam Sadovsky and Quoc Le and Trevor Strohman and Yonghui Wu and Slav Petrov and Jeffrey Dean and Oriol Vinyals},
  year          = {2025},
  eprint        = {2312.11805},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2312.11805}
}

@misc{chowdhery2022palmscalinglanguagemodeling,
  title         = {PaLM: Scaling Language Modeling with Pathways},
  author        = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  year          = {2022},
  eprint        = {2204.02311},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2204.02311}
}

@misc{sahabatAI2024news,
  title  = {Indosat Ooredoo Hutchison and GoTo Launch Sahabat-AI: Indonesia’s Open- Source LLM for Empowering Digital Sovereignty},
  author = {{GoTo Gojek Tokopedia} and {Indosat Ooredoo Hutchison}},
  year   = {2024},
  month  = nov,
  day    = {15},
  url    = {https://www.gotocompany.com/en/news/press/indosat-ooredoo-hutchison-and-goto-launch-sahabat-ai-indonesias-open-source-llm-for-empowering-digital-sovereignty?utm_source},
  note   = {Accessed: 2025-05-17}
}

@misc{ng2025sealionsoutheastasianlanguages,
  title         = {SEA-LION: Southeast Asian Languages in One Network},
  author        = {Raymond Ng and Thanh Ngan Nguyen and Yuli Huang and Ngee Chia Tai and Wai Yi Leong and Wei Qi Leong and Xianbin Yong and Jian Gang Ngui and Yosephine Susanto and Nicholas Cheng and Hamsawardhini Rengarajan and Peerat Limkonchotiwat and Adithya Venkatadri Hulagadri and Kok Wai Teng and Yeo Yeow Tong and Bryan Siow and Wei Yi Teo and Wayne Lau and Choon Meng Tan and Brandon Ong and Zhi Hao Ong and Jann Railey Montalan and Adwin Chan and Sajeban Antonyrex and Ren Lee and Esther Choa and David Ong Tat-Wee and Bing Jie Darius Liu and William Chandra Tjhi and Erik Cambria and Leslie Teo},
  year          = {2025},
  eprint        = {2504.05747},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2504.05747}
}

@misc{zhang2024seallms3openfoundation,
  title         = {SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for Southeast Asian Languages},
  author        = {Wenxuan Zhang and Hou Pong Chan and Yiran Zhao and Mahani Aljunied and Jianyu Wang and Chaoqun Liu and Yue Deng and Zhiqiang Hu and Weiwen Xu and Yew Ken Chia and Xin Li and Lidong Bing},
  year          = {2024},
  eprint        = {2407.19672},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2407.19672}
}

@misc{dou2024sailoropenlanguagemodels,
  title         = {Sailor: Open Language Models for South-East Asia},
  author        = {Longxu Dou and Qian Liu and Guangtao Zeng and Jia Guo and Jiahui Zhou and Wei Lu and Min Lin},
  year          = {2024},
  eprint        = {2404.03608},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2404.03608}
}

@online{weka2024rag,
  title  = {Retrieval Augmented Generation: Everything You Need to Know About RAG in AI},
  author = {{Weka.io}},
  year   = {2024},
  month  = {10},
  day    = {24},
  url    = {https://www.weka.io/learn/guide/ai-ml/retrieval-augmented-generation/},
  note   = {Accessed: 2025-05-18}
}

@online{nvidiaRAG,
  title  = {What Is Retrieval-Augmented Generation, aka RAG?},
  author = {{NVIDIA}},
  year   = {2025},
  month  = {1},
  day    = {31},
  url    = {https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/},
  note   = {Accessed: 2025-05-18}
}

@misc{xu2025simragselfimprovingretrievalaugmentedgeneration,
  title         = {SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains},
  author        = {Ran Xu and Hui Liu and Sreyashi Nag and Zhenwei Dai and Yaochen Xie and Xianfeng Tang and Chen Luo and Yang Li and Joyce C. Ho and Carl Yang and Qi He},
  year          = {2025},
  eprint        = {2410.17952},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2410.17952}
}

@online{harkar2025rag,
  author = {Shalini Harkar},
  title  = {RAG Techniques},
  year   = {2025},
  month  = feb,
  day    = {25},
  url    = {https://www.ibm.com/think/topics/rag-techniques},
  note   = {Accessed: 2025-05-18}
}

@misc{gao2024retrievalaugmentedgenerationlargelanguage,
  title         = {Retrieval-Augmented Generation for Large Language Models: A Survey},
  author        = {Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
  year          = {2024},
  eprint        = {2312.10997},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2312.10997}
}

@inproceedings{Ma_2023,
  title     = {Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!},
  url       = {http://dx.doi.org/10.18653/v1/2023.findings-emnlp.710},
  doi       = {10.18653/v1/2023.findings-emnlp.710},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  publisher = {Association for Computational Linguistics},
  author    = {Ma, Yubo and Cao, Yixin and Hong, Yong and Sun, Aixin},
  year      = {2023}
}

@online{kilpatrick2025gemini,
  author = {Logan Kilpatrick and Zach Gleicher and Parashar Shah},
  title  = {State-of-the-art text embedding via the Gemini API},
  year   = {2025},
  month  = {3},
  day    = {7},
  url    = {https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/},
  note   = {Accessed: 2025-05-18}
}

@misc{muennighoff2023mtebmassivetextembedding,
  title         = {MTEB: Massive Text Embedding Benchmark},
  author        = {Niklas Muennighoff and Nouamane Tazi and Loïc Magne and Nils Reimers},
  year          = {2023},
  eprint        = {2210.07316},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2210.07316}
}

@misc{enevoldsen2025mmtebmassivemultilingualtext,
  title         = {MMTEB: Massive Multilingual Text Embedding Benchmark},
  author        = {Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and Márton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemiński and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystrøm and Roman Solomatin and Ömer Çağatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafał Poświata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Björn Plüster and Jan Philipp Harries and Loïc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek Šuppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael Günther and Mengzhou Xia and Weijia Shi and Xing Han Lù and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},
  year          = {2025},
  eprint        = {2502.13595},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2502.13595}
}

@software{wongso2024indonesian,
  author    = {Wilson Wongso and Ananto Joyoadikusumo and David Samuel Setiawan and Steven Limcorn},
  title     = {LazarusNLP/indonesian-sentence-embeddings: v0.0.1},
  year      = {2024},
  month     = apr,
  day       = {17},
  version   = {v0.0.1},
  publisher = {Lazarus NLP},
  doi       = {10.5281/zenodo.10983756},
  url       = {https://github.com/LazarusNLP/indonesian-sentence-embeddings/tree/v0.0.1},
  abstract  = {Embedding Representation for Indonesian Sentences!},
  license   = {Apache-2.0}
}

@misc{jiang2024piperagfastretrievalaugmentedgeneration,
  title         = {PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design},
  author        = {Wenqi Jiang and Shuai Zhang and Boran Han and Jie Wang and Bernie Wang and Tim Kraska},
  year          = {2024},
  eprint        = {2403.05676},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2403.05676}
}

@misc{malkov2018efficientrobustapproximatenearest,
  title         = {Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs},
  author        = {Yu. A. Malkov and D. A. Yashunin},
  year          = {2018},
  eprint        = {1603.09320},
  archiveprefix = {arXiv},
  primaryclass  = {cs.DS},
  url           = {https://arxiv.org/abs/1603.09320}
}

@article{payong2024choosing,
  author  = {Adrien Payong and Shaoni Mukherjee},
  title   = {How to Choose the Right Vector Database for Your RAG Architecture},
  journal = {DigitalOcean Community},
  year    = {2024},
  month   = {12},
  day     = {4},
  url     = {https://www.digitalocean.com/community/conceptual-articles/how-to-choose-the-right-vector-database},
  note    = {Accessed: 2025-05-18}
}

@article{Zhang2025,
  author   = {Gongbo Zhang and Zihan Xu and Qiao Jin and Fangyi Chen and Yilu Fang and Yi Liu and Justin F. Rousseau and Ziyang Xu and Zhiyong Lu and Chunhua Weng and Yifan Peng},
  title    = {Leveraging long context in retrieval augmented language models for medical question answering},
  journal  = {npj Digital Medicine},
  year     = {2025},
  volume   = {8},
  number   = {1},
  pages    = {239},
  doi      = {10.1038/s41746-025-01651-w},
  url      = {https://doi.org/10.1038/s41746-025-01651-w},
  issn     = {2398-6352},
  abstract = {While holding great promise for improving and facilitating healthcare through applications of medical literature summarization, large language models (LLMs) struggle to produce up-to-date responses on evolving topics due to outdated knowledge or hallucination. Retrieval-augmented generation (RAG) is a pivotal innovation that improves the accuracy and relevance of LLM responses by integrating LLMs with a search engine and external sources of knowledge. However, the quality of RAG responses can be largely impacted by the rank and density of key information in the retrieval results, such as the “lost-in-the-middle” problem. In this work, we aim to improve the robustness and reliability of the RAG workflow in the medical domain. Specifically, we propose a map-reduce strategy, BriefContext, to combat the “lost-in-the-middle” issue without modifying the model weights. We demonstrated the advantage of the workflow with various LLM backbones and on multiple QA datasets. This method promises to improve the safety and reliability of LLMs deployed in healthcare domains by reducing the risk of misinformation, ensuring critical clinical content is retained in generated responses, and enabling more trustworthy use of LLMs in critical tasks such as medical question answering, clinical decision support, and patient-facing applications.}
}

@article{chirkin2024ivfpq,
  author  = {Artem Chirkin and Akira Naruse and Tamás Fehér and Yong Wang and Corey Nolet},
  title   = {Accelerating Vector Search: NVIDIA cuVS IVF-PQ Part 1, Deep Dive},
  journal = {NVIDIA Technical Blog},
  year    = {2024},
  month   = {July},
  day     = {18},
  url     = {https://developer.nvidia.com/blog/accelerating-vector-search-nvidia-cuvs-ivf-pq-deep-dive-part-1}
}

@inproceedings{10.1145/3644815.3644945,
  author    = {Barnett, Scott and Kurniawan, Stefanus and Thudumu, Srikanth and Brannelly, Zach and Abdelrazek, Mohamed},
  title     = {Seven Failure Points When Engineering a Retrieval Augmented Generation System},
  year      = {2024},
  isbn      = {9798400705915},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3644815.3644945},
  doi       = {10.1145/3644815.3644945},
  abstract  = {Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.},
  booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
  pages     = {194–199},
  numpages  = {6},
  keywords  = {retrieval augmented generation, RAG, SE4AI, case study},
  location  = {Lisbon, Portugal},
  series    = {CAIN '24}
}

@misc{zhou2024trustworthinessretrievalaugmentedgenerationsystems,
  title         = {Trustworthiness in Retrieval-Augmented Generation Systems: A Survey},
  author        = {Yujia Zhou and Yan Liu and Xiaoxi Li and Jiajie Jin and Hongjin Qian and Zheng Liu and Chaozhuo Li and Zhicheng Dou and Tsung-Yi Ho and Philip S. Yu},
  year          = {2024},
  eprint        = {2409.10102},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR},
  url           = {https://arxiv.org/abs/2409.10102}
}

@inbook{Yu_2025,
  title     = {Evaluation of Retrieval-Augmented Generation: A Survey},
  isbn      = {9789819610242},
  issn      = {1865-0937},
  url       = {http://dx.doi.org/10.1007/978-981-96-1024-2_8},
  doi       = {10.1007/978-981-96-1024-2_8},
  booktitle = {Big Data},
  publisher = {Springer Nature Singapore},
  author    = {Yu, Hao and Gan, Aoran and Zhang, Kai and Tong, Shiwei and Liu, Qi and Liu, Zhaofeng},
  year      = {2025},
  pages     = {102–120}
}

@misc{chen2023benchmarkinglargelanguagemodels,
  title         = {Benchmarking Large Language Models in Retrieval-Augmented Generation},
  author        = {Jiawei Chen and Hongyu Lin and Xianpei Han and Le Sun},
  year          = {2023},
  eprint        = {2309.01431},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2309.01431}
}

@inproceedings{wang-etal-2024-searching,
  title     = {Searching for Best Practices in Retrieval-Augmented Generation},
  author    = {Wang, Xiaohua  and
               Wang, Zhenghua  and
               Gao, Xuan  and
               Zhang, Feiran  and
               Wu, Yixin  and
               Xu, Zhibo  and
               Shi, Tianyuan  and
               Wang, Zhengyuan  and
               Li, Shizheng  and
               Qian, Qi  and
               Yin, Ruicheng  and
               Lv, Changze  and
               Zheng, Xiaoqing  and
               Huang, Xuanjing},
  editor    = {Al-Onaizan, Yaser  and
               Bansal, Mohit  and
               Chen, Yun-Nung},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2024},
  address   = {Miami, Florida, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.emnlp-main.981/},
  doi       = {10.18653/v1/2024.emnlp-main.981},
  pages     = {17716--17736},
  abstract  = {Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a {\textquotedblleft}retrieval as generation{\textquotedblright} strategy.}
}

@misc{brehme2025llmstrustedevaluatingrag,
  title         = {Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets},
  author        = {Lorenz Brehme and Thomas Ströhle and Ruth Breu},
  year          = {2025},
  eprint        = {2504.20119},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR},
  url           = {https://arxiv.org/abs/2504.20119}
}

@misc{an2025ragllmssafersafety,
  title         = {RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models},
  author        = {Bang An and Shiyue Zhang and Mark Dredze},
  year          = {2025},
  eprint        = {2504.18041},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2504.18041}
}

@misc{bucher2024finetunedsmallllmsstill,
  title         = {Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in Text Classification},
  author        = {Martin Juan José Bucher and Marco Martini},
  year          = {2024},
  eprint        = {2406.08660},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2406.08660}
}

@misc{hu2021loralowrankadaptationlarge,
  title         = {LoRA: Low-Rank Adaptation of Large Language Models},
  author        = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  year          = {2021},
  eprint        = {2106.09685},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2106.09685}
}

@misc{dettmers2023qloraefficientfinetuningquantized,
  title         = {QLoRA: Efficient Finetuning of Quantized LLMs},
  author        = {Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
  year          = {2023},
  eprint        = {2305.14314},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2305.14314}
}

@misc{parthasarathy2024ultimateguidefinetuningllms,
  title         = {The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities},
  author        = {Venkatesh Balavadhani Parthasarathy and Ahtsham Zafar and Aafaq Khan and Arsalan Shahid},
  year          = {2024},
  eprint        = {2408.13296},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2408.13296}
}

@misc{hu2023llmadaptersadapterfamilyparameterefficient,
  title         = {LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models},
  author        = {Zhiqiang Hu and Lei Wang and Yihuai Lan and Wanyu Xu and Ee-Peng Lim and Lidong Bing and Xing Xu and Soujanya Poria and Roy Ka-Wei Lee},
  year          = {2023},
  eprint        = {2304.01933},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2304.01933}
}

@inproceedings{hao-etal-2024-meft,
  title     = {{MEFT}: Memory-Efficient Fine-Tuning through Sparse Adapter},
  author    = {Hao, Jitai  and
               Sun, Weiwei  and
               Xin, Xin  and
               Meng, Qi  and
               Chen, Zhumin  and
               Ren, Pengjie  and
               Ren, Zhaochun},
  editor    = {Ku, Lun-Wei  and
               Martins, Andre  and
               Srikumar, Vivek},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = aug,
  year      = {2024},
  address   = {Bangkok, Thailand},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.acl-long.129/},
  doi       = {10.18653/v1/2024.acl-long.129},
  pages     = {2375--2388},
  abstract  = {Parameter-Efficient Fine-tuning (PEFT) facilitates the fine-tuning of Large Language Models (LLMs) under limited resources. However, the fine-tuning performance with PEFT on complex, knowledge-intensive tasks is limited due to the constrained model capacity, which originates from the limited number of additional trainable parameters. To overcome this limitation, we introduce a novel mechanism that fine-tunes LLMs with adapters of larger size yet memory-efficient. This is achieved by leveraging the inherent activation sparsity in the Feed-Forward Networks (FFNs) of LLMs and utilizing the larger capacity of Central Processing Unit (CPU) memory compared to Graphics Processing Unit (GPU). We store and update the parameters of larger adapters on the CPU. Moreover, we employ a Mixture of Experts (MoE)-like architecture to mitigate unnecessary CPU computations and reduce the communication volume between the GPU and CPU. This is particularly beneficial over the limited bandwidth of PCI Express (PCIe). Our method can achieve fine-tuning results comparable to those obtained with larger memory capacities, even when operating under more limited resources such as a 24GB memory single GPU setup, with acceptable loss in training efficiency. Our codes are available at https://github.com/CURRENTF/MEFT.}
}

@misc{wang2022adamixmixtureofadaptationsparameterefficientmodel,
  title         = {AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning},
  author        = {Yaqing Wang and Sahaj Agarwal and Subhabrata Mukherjee and Xiaodong Liu and Jing Gao and Ahmed Hassan Awadallah and Jianfeng Gao},
  year          = {2022},
  eprint        = {2205.12410},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2205.12410}
}


@inproceedings{pmlr-v188-bansal22a,
  title     = {Meta-Adapters: Parameter Efficient Few-shot Fine-tuning through Meta-Learning},
  author    = {Bansal, Trapit and Alzubi, Salaheddin and Wang, Tong and Lee, Jay-Yoon and McCallum, Andrew},
  booktitle = {Proceedings of the First International Conference on Automated Machine Learning},
  pages     = {19/1--18},
  year      = {2022},
  editor    = {Guyon, Isabelle and Lindauer, Marius and van der Schaar, Mihaela and Hutter, Frank and Garnett, Roman},
  volume    = {188},
  series    = {Proceedings of Machine Learning Research},
  month     = {25--27 Jul},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v188/bansal22a/bansal22a.pdf},
  url       = {https://proceedings.mlr.press/v188/bansal22a.html},
  abstract  = {Consistent improvements in the representational capacity of large pre-trained transformers has made it increasingly viable to serve these models as shared priors that can be fine-tuned on a large number of downstream tasks. However, fine-tuning the entire model for every task of interest makes a copy of all the model parameters, rendering such scenarios highly impractical.  Recently introduced Adapter methods propose a promising alternative, one where only a small number of additional parameters are introduced per task specifically for fine-tuning. However, Adapters often require large amounts of task-specific data for good performance and don’t work well in data-scarce few-shot scenarios. In this paper, we approach parameter-efficient fine-tuning in few-shot settings from a meta-learning perspective. We introduce Meta-Adapters, which are small blocks of meta-learned adapter layers inserted in a pre-trained model that re-purpose a frozen pre-trained model into a parameter-efficient few-shot learner. Meta-Adapters perform competitively with state-of-the-art few-shot learning methods that require full fine-tuning, while only fine-tuning 0.6% of the parameters. We evaluate Meta-Adapters along with multiple transfer learning baselines on an evaluation suite of 17 classification tasks and find that they improve few-shot accuracy by a large margin over competitive parameter-efficient methods, while requiring significantly lesser parameters for fine-tuning. Moreover, when comparing few-shot prompting of GPT-3 against few-shot fine-tuning with Meta-Adapters, we find that Meta-Adapters perform  competitively while working with pre-trained transformers that are many orders of magnitude (1590{\texttimes}) smaller in size than GPT-3.}
}

@inproceedings{li-liang-2021-prefix,
  title     = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author    = {Li, Xiang Lisa  and
               Liang, Percy},
  editor    = {Zong, Chengqing  and
               Xia, Fei  and
               Li, Wenjie  and
               Navigli, Roberto},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  month     = aug,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.acl-long.353/},
  doi       = {10.18653/v1/2021.acl-long.353},
  pages     = {4582--4597},
  abstract  = {Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {\textquotedblleft}virtual tokens{\textquotedblright}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.}
}

@inproceedings{xia-etal-2024-rule,
  title     = {{RULE}: Reliable Multimodal {RAG} for Factuality in Medical Vision Language Models},
  author    = {Xia, Peng  and
               Zhu, Kangyu  and
               Li, Haoran  and
               Zhu, Hongtu  and
               Li, Yun  and
               Li, Gang  and
               Zhang, Linjun  and
               Yao, Huaxiu},
  editor    = {Al-Onaizan, Yaser  and
               Bansal, Mohit  and
               Chen, Yun-Nung},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2024},
  address   = {Miami, Florida, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.emnlp-main.62/},
  doi       = {10.18653/v1/2024.emnlp-main.62},
  pages     = {1081--1093},
  abstract  = {The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has enhanced medical diagnosis. However, current Med-LVLMs frequently encounter factual issues, often generating responses that do not align with established medical facts. Retrieval-Augmented Generation (RAG), which utilizes external knowledge, can improve the factual accuracy of these models but introduces two major challenges. First, limited retrieved contexts might not cover all necessary information, while excessive retrieval can introduce irrelevant and inaccurate references, interfering with the model`s generation. Second, in cases where the model originally responds correctly, applying RAG can lead to an over-reliance on retrieved contexts, resulting in incorrect answers. To address these issues, we propose RULE, which consists of two components. First, we introduce a provably effective strategy for controlling factuality risk through the calibrated selection of the number of retrieved contexts. Second, based on samples where over-reliance on retrieved contexts led to errors, we curate a preference dataset to fine-tune the model, balancing its dependence on inherent knowledge and retrieved contexts for generation. We demonstrate the effectiveness of RAFE on three medical VQA datasets, achieving an average improvement of 20.8{\%} in factual accuracy.}
}

@misc{chen2025ruleragruleguidedretrievalaugmentedgeneration,
  title         = {RuleRAG: Rule-Guided Retrieval-Augmented Generation with Language Models for Question Answering},
  author        = {Zhongwu Chen and Chengjin Xu and Dingmin Wang and Zhen Huang and Yong Dou and Xuhui Jiang and Jian Guo},
  year          = {2025},
  eprint        = {2410.22353},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR},
  url           = {https://arxiv.org/abs/2410.22353}
}

@inproceedings{NEURIPS2023_ac662d74,
  author    = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and YU, LILI and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages     = {55006--55021},
  publisher = {Curran Associates, Inc.},
  title     = {LIMA: Less Is More for Alignment},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/ac662d74829e4407ce1d126477f4a03a-Paper-Conference.pdf},
  volume    = {36},
  year      = {2023}
}

@misc{lin2024raditretrievalaugmenteddualinstruction,
  title         = {RA-DIT: Retrieval-Augmented Dual Instruction Tuning},
  author        = {Xi Victoria Lin and Xilun Chen and Mingda Chen and Weijia Shi and Maria Lomeli and Rich James and Pedro Rodriguez and Jacob Kahn and Gergely Szilvasy and Mike Lewis and Luke Zettlemoyer and Scott Yih},
  year          = {2024},
  eprint        = {2310.01352},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2310.01352}
}

@misc{siriwardhana2021finetuneentireragarchitecture,
  title         = {Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering},
  author        = {Shamane Siriwardhana and Rivindu Weerasekera and Elliott Wen and Suranga Nanayakkara},
  year          = {2021},
  eprint        = {2106.11517},
  archiveprefix = {arXiv},
  primaryclass  = {cs.IR},
  url           = {https://arxiv.org/abs/2106.11517}
}

@misc{wu2025retrievalaugmentedgenerationnaturallanguage,
  title         = {Retrieval-Augmented Generation for Natural Language Processing: A Survey},
  author        = {Shangyu Wu and Ying Xiong and Yufei Cui and Haolun Wu and Can Chen and Ye Yuan and Lianming Huang and Xue Liu and Tei-Wei Kuo and Nan Guan and Chun Jason Xue},
  year          = {2025},
  eprint        = {2407.13193},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2407.13193}
}

@article{Lakatos_2025,
  title     = {Investigating the Performance of Retrieval-Augmented Generation and Domain-Specific Fine-Tuning for the Development of AI-Driven Knowledge-Based Systems},
  volume    = {7},
  issn      = {2504-4990},
  url       = {http://dx.doi.org/10.3390/make7010015},
  doi       = {10.3390/make7010015},
  number    = {1},
  journal   = {Machine Learning and Knowledge Extraction},
  publisher = {MDPI AG},
  author    = {Lakatos, Róbert and Pollner, Péter and Hajdu, András and Joó, Tamás},
  year      = {2025},
  month     = feb,
  pages     = {15}
}

@misc{balaguer2024ragvsfinetuningpipelines,
  title         = {RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture},
  author        = {Angels Balaguer and Vinamra Benara and Renato Luiz de Freitas Cunha and Roberto de M. Estevão Filho and Todd Hendry and Daniel Holstein and Jennifer Marsman and Nick Mecklenburg and Sara Malvar and Leonardo O. Nunes and Rafael Padilha and Morris Sharp and Bruno Silva and Swati Sharma and Vijay Aski and Ranveer Chandra},
  year          = {2024},
  eprint        = {2401.08406},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2401.08406}
}

@misc{alghisi2024finetuneragevaluatingdifferent,
  title         = {Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue},
  author        = {Simone Alghisi and Massimo Rizzoli and Gabriel Roccabruna and Seyed Mahed Mousavi and Giuseppe Riccardi},
  year          = {2024},
  eprint        = {2406.06399},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2406.06399}
}

@misc{ovadia2024finetuningretrievalcomparingknowledge,
  title         = {Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs},
  author        = {Oded Ovadia and Menachem Brief and Moshik Mishaeli and Oren Elisha},
  year          = {2024},
  eprint        = {2312.05934},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2312.05934}
}

@misc{govtech2016askjamie,
  author       = {{GovTech Singapore}},
  title        = {Winning by Innovating},
  year         = {2016},
  howpublished = {\url{https://www.tech.gov.sg/media/technews/winning-by-innovating}},
  note         = {Accessed May 2025}
}

@misc{govtech2025pair,
  author       = {{GovTech Singapore}},
  title        = {Pair - Productivity AI Assistant for Public Officers},
  year         = {2025},
  howpublished = {\url{https://www.tech.gov.sg/products-and-services/for-government-agencies/productivity-and-marketing/pair}},
  note         = {Accessed May 2025}
}

@article{chong2024peach,
  author  = {Chong, S. and others},
  title   = {PEACH: AI Chatbot for Perioperative Medicine},
  journal = {arXiv preprint arXiv:2412.18096},
  year    = {2024},
  url     = {https://arxiv.org/abs/2412.18096},
  note    = {Accessed May 2025}
}

@misc{nii2025ai,
  author       = {{National Institute of Informatics}},
  title        = {Japan unveils generative AI that passes national medical exam},
  year         = {2025},
  howpublished = {\url{https://www.aa.com.tr/en/artificial-intelligence/japan-unveils-generative-ai-that-passes-national-medical-exam/3551960}},
  note         = {Accessed: 2025-05-18}
}

@misc{crds2024rwd,
  author       = {{Center for Research and Development Strategy (CRDS)}},
  title        = {Health \& Medical Real World Data Infrastructure - Catalyst of Generative AI Development in Japan - CRDS-FY2023-SP-04},
  year         = {2024},
  howpublished = {\url{https://www.jst.go.jp/crds/en/publications/CRDS-FY2023-SP-04.html}},
  note         = {Accessed: 2025-05-18}
}
